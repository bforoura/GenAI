{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8buiBAaIT2JgRjKV7fitW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bforoura/GenAI/blob/main/Module8/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT-2 Chatbot Implementation**\n",
        "\n",
        "* This is an implementation of a simple chatbot based on OpenAI's GPT-2 model.\n",
        "\n",
        "* GPT-2 is a language model capable of generating human-like text based on a given input.\n",
        "\n",
        "* Hugging Face offers a wide variety of pre-trained models, including GPT-2, that are available for download and use locally without the need for an **API key**.\n",
        "\n",
        "* If you're using **Hugging Face’s Inference API** to make model predictions directly on **their servers** (i.e., not downloading and running the model locally), then you would need an API key to authenticate and access the API for hosting models remotely.\n",
        "\n",
        "* The **chatbot** is designed to interact with users, generate contextually relevant responses, and provide an engaging conversational experience.\n",
        "\n",
        "\n",
        "* The code uses the transformers library from **Hugging Face** to load the pre-trained GPT-2 model and tokenizer, and PyTorch is used for handling the model’s operations.\n",
        "\n",
        "\n",
        "* The chatbot supports features like **adjustable response length**, **temperature sampling**, and **top-k** or **top-p** sampling for more diverse and creative responses.\n",
        "\n",
        "* The **pre-trained GPT-2 model** and its corresponding tokenizer are loaded using the from_pretrained method from the transformers library.\n",
        "\n",
        "* The tokenizer is responsible for converting human-readable text into a format (tokens) that the model can understand, and vice versa.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Sampling Parameters**\n",
        "\n",
        "* To make the chatbot's responses more natural and less repetitive, several sampling techniques are used:\n",
        "\n",
        ">>* **Temperature**: This parameter controls the randomness of the text generation. A higher temperature (e.g., 1.0) introduces more randomness and creativity, while a lower temperature (e.g., 0.2) makes the output more deterministic and focused.\n",
        "\n",
        ">>* **Top-k Sampling**: This restricts the model to choose the next token from the **top k most likely options**, preventing it from picking less likely words that might cause nonsensical output.\n",
        "\n",
        ">>* **Top-p Sampling (Nucleus Sampling)**: Instead of limiting the next token choices to the top k tokens, top-p considers the **smallest set of tokens whose cumulative probability exceeds the threshold p**, thus ensuring that the most probable, yet **diverse tokens** are chosen.\n",
        "\n",
        "\n",
        "\n",
        "**Challenges and Limitations**\n",
        "\n",
        "* **Repetition**\n",
        ">* Initially, the chatbot might generate repetitive responses due to greedy decoding (always picking the most probable next word).\n",
        ">* However, by enabling sampling and adjusting parameters like temperature, top-k, and top-p, the repetition is reduced, and the responses can become more varied and creative.\n",
        "\n",
        "* **Context Awareness**\n",
        ">* GPT-2 can generate coherent responses based on a single input but lacks long-term context awareness.\n",
        ">* In a real-world conversation, where the context from previous interactions is important, the model might struggle to maintain a consistent conversation.\n",
        ">* This limitation can be addressed by **fine-tuning the model** on domain-specific conversations or using a more sophisticated model like GPT-3 or GPT-4.\n",
        "\n",
        "* **Response Quality**\n",
        ">* While GPT-2 produces human-like responses, the quality of its responses can sometimes be inconsistent.\n",
        ">* Fine-tuning or incorporating additional techniques, such as reinforcement learning from human feedback (RLHF), can improve the quality and relevance of the responses."
      ],
      "metadata": {
        "id": "GAqtV-91de6k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH23xKNSZKT3",
        "outputId": "8652eb9b-fc42-4148-f07e-fbdb7486d488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load a larger version of GPT-2\n",
        "model_name = \"gpt2-large\"  # Change this to gpt2-medium, gpt2-large, or gpt2-xl for bigger models\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def generate_response(prompt, max_length=100, temperature=0.7, top_k=50, top_p=0.9):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Create attention mask\n",
        "    # This means that initially, the attention mask is set to 1 for every token,\n",
        "    # signaling that the model should attend to all tokens\n",
        "    attention_mask = torch.ones(input_ids.shape, device=input_ids.device)\n",
        "\n",
        "\n",
        "    # Generate response with attention mask and added parameters\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=50256,\n",
        "            do_sample=True  # Enable sampling to use temperature, top_k, and top_p\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "print(\"Chatbot: Hi there! How can I help you?\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    response = generate_response(user_input)\n",
        "    print(\"Chatbot:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "zIODlm1eZOtu",
        "outputId": "09639dca-7d18-4ba4-b114-a2392f3629e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: Hi there! How can I help you?\n",
            "You: How's it going?\n",
            "Chatbot: How's it going? Well, we have a little bit of a problem. We have a lot of people who have been waiting in line. They've been waiting for over two hours. So I'm going to have to cut the line down. I've just got to get it down to two. I'm going to have to do that. But I'm going to have to get it down to two.\n",
            "\n",
            "I know that there are a lot of people that are here for the first\n",
            "You: The pink elephant sang a loud note.\n",
            "Chatbot: The pink elephant sang a loud note.\n",
            "\n",
            "\"The green elephant sang a loud note.\"\n",
            "\n",
            "\"The blue elephant sang a loud note.\"\n",
            "\n",
            "\"The yellow elephant sang a loud note.\"\n",
            "\n",
            "\"The red elephant sang a loud note.\"\n",
            "\n",
            "\"The yellow elephant sang a loud note.\"\n",
            "\n",
            "\"The red elephant sang a loud note.\"\n",
            "\n",
            "\"The yellow elephant sang a loud note.\"\n",
            "\n",
            "\"The red elephant sang a loud note.\"\n",
            "\n",
            "\"The\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-67a5d83ca640>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot: Hi there! How can I help you?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot: Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UvQT4oxKc3xN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}